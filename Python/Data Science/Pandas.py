import pandas as pd
import numpy as np

# Создание структуры с помощью списков
# s1 = pd.Series([1, 2, 3, 4, 5], ["A", "B", "C", "D", "E"], dtype="float64", copy=True)
# print(s1)

# Создание структуры с помощью numpy array
# ndarr = np.array([1, 2, 3, 4, 5])
# print(ndarr)
# print(type(ndarr))
# s2 = pd.Series(ndarr, ["First", "Second", "Third", "Fourth", "Fifth"], dtype="int64")
# print(s2)

# Создание структуры с помощью словаря, где ключи станут метками, а значения - знаениями.
# d = {'a': 1, 'b': 2, 'c': 3}
# s3 = pd.Series(d)
# print(s3)

# Обращение к элементам с помщью метки
# print(s3["a"])
# Доступен синтаксис работы со срезами
# print(s3[:2])
# # В поле для индекса можно поместить условное выражение
# print(s3[s3 <= 2])

# s4 = pd.Series([10, 20, 30, 40, 50], ['a', 'b', 'c', 'd', 'e'])
# s6 = pd.Series([1, 2, 3, 4, 5], ['a', 'b', 'c', 'd', 'e'])

# Со структурами Series можно работать как с вектораи: складывать, умножать вектор на число и т.п.
# print(s6 + s4)

# Создание структуры DataFrame с помощью словаря
# d = {'price': pd.Series([1, 2, 3], index=['v1', 'v2', 'v3']), 'count': pd.Series([10, 12, 7], index=['v1', 'v2', 'v3']) }
# df1 = pd.DataFrame(d)
# print(df1)

# Индексы созданного DataFrame
# print(df1.index)

# Стобцы созданного DataFrame
# print(df1.columns)

# Аналогичный словарь, но на элементах ndarray
# d2 = {'price': np.array([1, 2, 3]), 'count': np.array([10, 12, 7])}
# df2 = pd.DataFrame(d2, index=['v1', 'v2', 'v3'])
# print(df2)

# Создание DataFrame из списка словарей
# d3 = [{'price': 3, 'count': 8}, {'price': 4, 'count': 11}]
# df3 = pd.DataFrame(d3)
# print(df3)
# print()
# # Информация по созданному DataFrame
# # print(df3.info())

# Создание DataFrame из двумерного массива
# nda1 = np.array([[1, 2, 3], [10, 20, 30]])
# df4 = pd.DataFrame(nda1)
# print(df4)


# d = {'count': [1, 2, 3, 4, 5], 'volume': [10, 20, 30, 40, 50]}
# df1 = pd.DataFrame(d, index=['v1', 'v2', 'v3', 'v4', 'v5'])
# print(df1)


# Операция: выбор столбца
# print(df1["count"])

# Операция:выбор строки по метке
# print(df1.loc["v1"])

# Операция: выбор строки по индексу
# print(df1.iloc[2])

# Операция: срез по строкам
# print(df1[0:3])

# Операция: выбор строк, отвечающих условию
# print(df1[df1["volume"] >= 30])


# s = pd.Series([10, 20, 30, 40, 50], ['a', 'b', 'c', 'd', 'e'])
#
# d = {"price": [1, 2, 3], 'count': [10, 20, 30], 'percent': [24, 51, 71]}
# df = pd.DataFrame(d, index=['a', 'b', 'c'])


# Доступ к данным структуры Series
# Доступ с использованием меток

# Обращение по отдельной метке
# print(s['a'])

# Обращение по массиву меток
# print(s[['a', 'c', 'e']])

# Обращение по срезу меток
# print(s['c':'e'])


# Доступ с использовнием целочисленных индексов

# Обращение по отдельному индексу
# print(s[1])
# print(s.iloc[1])

# Обращение с использованием списка индексов
# print(s[[1, 2, 3]])
# print(s.iloc[[1, 2, 3]])

# Обращение по срезу индексов
# print(s[:3])


# Доступ с использованием callable-функции

# Элементы, значение которых больше либо равно 30
# print(s[lambda x: x >= 30])


# Доступ с использованием логического выражения

# Элементы, значение которых больше 30
# print(s[s > 30])


# Доступ к данным структуры DataFrame
# Доступ с использованием меток

# Обращение к конкретному столбцу
# print(df['count'])

# Обращение с использованием массива стобцов
# print(df[["count", "percent"]])

# Обращение по срезу меток
# print(df['a':'b'])


# Доступ с использованием callable-функции
# Получим все элементы, у которых значение в столбце "percent" больше либо равно 25
# print(df[lambda x : x['percent'] >= 25])


# Доступ с использованием логического выражения
# print(df[df['price'] >= 2])


# Использование атрибутов для доступа к данным

# s = pd.Series([10, 20, 30, 40, 50], ['a', 'b', 'c', 'd', 'e'])

# Для доступа к элементу через атрибут необходимо указать его через точку после имени переменной
# print(s.a)


# d = {"price": [1, 2, 3], 'count': [10, 20, 30], 'percent': [24, 51, 71]}
# df = pd.DataFrame(d, index=['a', 'b', 'c'])

# Получим доступ к столбцу "price"
# print(df.price)


# Получение случайного набора из структуры pandas

# s = pd.Series([10, 20, 30, 40, 50], ['a', 'b', 'c', 'd', 'e'])

# Для выбора случайного элемента из Series
# print(s.sample())

# Можно сделать выборку из нескольких элементов, для этого нужно передать количество элементов через параметр n
# print(s.sample(n=3))

# Доля от общего числа объектов в структуре
# print(s.sample(frac=0.6))


# Дополнительно, в качестве аргумента, мы можем передать вектов весов, длина которого
# должна быть равна количетсву элементов в структуре,а сумма элементов вектора - единице.
# Вес, в данном случае, это вероятность появления элемента в выборке.

# w = [0.1, 0.2, 0.5, 0.1, 0.1]
# print(s.sample(n=3, weights=w))


# d = {'price': [1, 2, 3, 5, 6], 'count': [10, 20, 30, 40, 50], 'percent': [24, 51, 71, 25, 42]}
# df = pd.DataFrame(d)

# print(df.sample())
# print(df.sample(n=2))

# Для того, чтобы выбрать столбец случайным образом
# print(df.sample(axis=1))

# Выбор двух столбцов случайным образом
# print(df.sample(n=2, axis=1))


# Индексация с использованием логических выражений

# s = pd.Series([10, 20, 30, 40, 50, 10, 10], ['a', 'b', 'c', 'd', 'e', 'f', 'g'])

# print(s[s > 30])

# Получим все элементы структуры, значение которых равно 10
# print(s[s == 10])


# Элементы структуры, значения которых находятся в интервале [30, 50)
# print(s[(s >= 30) & (s < 50)])


# При работе с DataFrame необходимо указывать столбец, по-которому будет производиться выборка

# d = {'price': [1, 2, 3, 5, 6], "count": [10, 20, 30, 40, 50], 'percent': [24, 51, 71, 25, 42],
#      "cat": ["A", "B", "A", "A", "C"]}
# df = pd.DataFrame(d)
# print(df)


# Выделим список строк таблицы, у которых значение в поле "price" больше, чем 3
# print(df[df["price"] > 3])


# В качестве логического выражения можно использовать довольно сложные конструкции
# с применением функций map, filter, lambda-выражений и т.п.

# fn = df['cat'].map(lambda x: x == "A")
# print(df[fn])


# Использование isin при работе с данными в pandas

# s = pd.Series([10, 20, 30, 40, 50, 10, 10], ['a', 'b', 'c', 'd', 'e', 'f', 'g'])

# Если значения элемента исходной структуры находится в списке [10, 20], то значение элемента равно True
# print(s.isin([10, 20]))


# d = {'price': [1, 2, 3, 5, 6], "count": [10, 20, 30, 40, 50], 'percent': [24, 51, 71, 25, 42]}
# df = pd.DataFrame(d)

# Если значение элемента исходной структуры находится в списке, то значение элемента в новой структуре равно True

# print(df.isin([1, 3, 25, 30, 10]))


# Типы данных в pandas
#
# s = pd.Series([1, 2, 3])
# print(s)

# d = [{'name': 'pen', 'price': 3.9, 'count': 8}, {'name': 'book',
#                                                  'price': 4.5, 'count': 11}]
#
# df = pd.DataFrame(d)
# print(df)
# print(df.info())


# Типы данных
# int64 - 64 разрядное целочисленное значение, не зависит от платформы
# float64 - 64 разрядное число с плавающей точкой, не зависит от платформы
# object - текст или любое другое значение
# bool - Булевое значение
# category - Конечное множество текстовых элементов
# datetime64[ns] - Дата / Время
# timedelta64[ns] - Разница между двумя datetime элементами


# Типы расширений pandas

# DatetimeTZDtype - datetime с поддержкой часового пояса
# CategoricalDtype - Тип для категориальных данных
# PeriodDtype - Тип для работы с периодическими данными
# SparseDtype - Тип для работы с разреженными данными
# IntervaDtype - Тип для работы с интервальными данными


# Инструменты для работы с типами

# astype(self, dtype, copy=True, errors="raise", **kwargs)
# dtype: тип данных или словарь с именами столбцов
# copy: bool
# astype будет возвращать копию структуры, если параметр равен True, в ином случае будет модифицироваться текущая структура
# errors: {'raise', 'ignore'};
# управляет процессом выброса исключений
# **kwargs
# Аргументы для передачи конструктору


# Преобразование типов для структуры Series

s = pd.Series([1, 2, 3])
# print(s.dtype)

# print(s.astype("float64"))

# d = [{'name': 'pen', 'price': 3.9, 'count': 8}, {'name': 'book', 'price': 4.5, 'count': 11}]
# df = pd.DataFrame(d)
# print(df.dtypes)

# Приведем тип поля 'count' к int32
# df['count'] = df['count'].astype("int32")
# print(df.dtypes)

# # Вернем прежний тип
# df = df.astype({'count': 'int64'})
# print(df.dtypes)

# Вспомогательные функции
# to_numeric() - преобразует данные в числовой тип
# to_datetime() - преобразует данные в тип datetime
# to_timedelta() - преобразует данные в тип timedelta


# Выборка данных по типу

# Для выборки данных по типу используется функция """"select_dtypes()"""""", она возвращает DataFrame, построенные
# из исходного DataFrame'a, в который будут входить стобцы с типами, указанными в аргументе include
# и не будут входить стобцы, типы которых, перечислены в exclude аргументе

# d = {"price": [1, 2, 3, 4, 5], "percent": [21, 51, 71, 25, 65], "pen": ["red", "blue", "yellow", "green", "black"]}

# df = pd.DataFrame(d)
# print(df)
# df = df.astype({"percent": "float64"})
# print(df)

# print(df.select_dtypes(exclude=["int64", "float64"]))


# Категориальные типы

# Создание структуры с набором категориальных данных
# Работа со структурой Series

# s = pd.Series(["r", "r", 'g', 'b'])
# print(s)

# Если необходимо явно указать, что элементы относятся к категориальному типу, то это нужно сделать через аргумент dtype:

# s = pd.Series(['r', 'r', 'g', 'b'], dtype='category')
# print(s)


# Работа с типом Categorical
# Если заранее известна структура категории: набор ее элементов и порядок, то можно создать объект класса Categorical

# pd.Categorical(values, categories=None, ordered=None, dtype=None, fastpath=False)
# values: список
# Элементы данных. Если дополнительно указывается категория через параметр categories,
# то значеня не из категории заменяются на NaN

# categories: набор уникальных элементов
# Задает набор значений, которые может принимать элемент категории. Если равен None,
# то категория строится по набору уникальных элементов из параметра values

# ordered : bool
# Определяет, является категория порядковой или нет. Если значение равно True,
# то категория является порядковой

# dtype : CategoricalDtype


# Создадим категорию

# colors = pd.Categorical(['r', 'g', 'g', 'b', 'r'])
# print(colors)

# colors = pd.Categorical(['r', 'g', 'g', 'b', 'r'], categories=['r', 'g', 'b'])
# print(colors)

# Если в наборе данных будут присутствовать элементы, которые не входят в категорию, им будут присвоены значения NaN:

# colors = pd.Categorical(['r', 'g', 'g', 'b', 'r', 'y', 'o'], categories=['r', 'g', 'b'])
# print(colors)

# Из объекта Categorical можно построить структуру Series:

# colors_s = pd.Series(colors)
# print(colors_s)

# Для очистки данных используется функция dropna()
# colors_s = pd.Series(colors).dropna()
# print(colors_s)


# Работа со структурой DataFrame

# df = pd.DataFrame({"C1": list('rrg'), "C2": list('rgb')}, dtype="category")
# print(df)
# print(df.dtypes)


# Порядковые категории
# Категориальные данные, о которых шла речь выше, не включают в себя
# отношение порядка, то есть для переменных, принимающих значения из
# таких категорий невозможно выполнить сравнение “больше-меньше”.
# Существуют категории, для которых отношение порядка задается, это
# может быть роль в фильме, образование и т.п.Категориальные данные, о которых шла речь выше, не включают в себя
# отношение порядка, то есть для переменных, принимающих значения из
# таких категорий невозможно выполнить сравнение “больше-меньше”.
# Существуют категории, для которых отношение порядка задается, это
# может быть роль в фильме, образование и т.п.

# level = pd.Categorical(['h', 'h', 'm', 'l'], categories=['l', 'm', 'h'], ordered=True)
# print(level)

# Обратите внимание на последнюю строчку, в ней указано как
# соотносятся между собой значения элементов категории в отношении
# “больше-меньше”. Если мы не укажем параметр ordered=True, то для
# такого набора данных нельзя будет выполнить поиск минимального и
# максимального элемента:

# c_var = pd.Series(pd.Categorical(['r', 'g', 'g', 'b', 'r'], categories=['r', 'g', 'b'], ordered=False))
# print(c_var)
# print(c_var.min())

# lev_var = pd.Series(level)
# print("min: {}, max: {}".format(lev_var.min(), lev_var.max()))

# Для непорядковых категорий запрещены сравнения на уровне объектов
# Если объекты будут содержать данные с элементами типа порядковая
# категория, то все будет выполнено корректно


# Работа с пропусками данных

# Pandas и отсутствующие данные

# import pandas as pd
# from io import StringIO

# data = "price,count,percent\n1,10,\n2,20,51\n3,30,"
# df = pd.read_csv(StringIO(data))
# print(df)

# df.loc[3] = {"price": 4, "count": None, "percent": 26.3}
# print(df)


# Для начала обратимся к методам из библиотеки pandas, которые
# позволяют быстро определить наличие элементов NaN в структурах

# print(pd.isnull(df))

# В результате мы получаем таблицу того же размера, но на месте
# реальных данных, в ней находятся элементы типа bool, которые равны
# False, если значение поля не-NaN, либо True в противном случае.


# Замена отсутствующих данных

# Отсутствующие данные объектов можно заменить на конкретные
# числовые значения с помощью метода fillna().

# print(df.fillna(0))

# Этот метод не изменяет текущую структуру, он возвращает структуру
# DataFrame, созданную на базе существующей, с заменой NaN значений
# на те, что переданы в метод в качестве аргумента.


# Данные можно заполнить средним значением по столбцу:

# print(df.fillna(df.mean()))


# Удаление объектов/столбцов с отсутсвующими данными

# Для того, чтобы удалить все объекты, которые содержат значения NaN
# воспользуйтесь методом dropna() без аргументов

# print(df.dropna())

# Вместо записей, можно удалить поля, для этого нужно вызвать метод
# dropna() с аргументом axis=1:

# print(df.dropna(axis=1))

# Pandas позволяет задать порог на количество не-NaN элементов. В
# приведенном ниже примере будут удалены все столбцы в которых
# количество не-NaN элементов меньше трех

# print(df.dropna(axis=1, thresh=3))


# Работа со структурами данных в pandas:
# удаление, объединение, расширение, группировка

# Добавление элементов в структуру pandas

# s = pd.Series([1, 2, 3, 4, 5], ["A", "B", "C", "D", "E"])

# d = {"color": ['red', 'green', 'blue'], 'speed': [56, 24, 65], "volume": [80, 65, 50]}

# df = pd.DataFrame(d)
# print(df)


# Добавление в Series

# Добавим в структуру s новый элемент с индексом 'F' и значением 6

# s['F'] = 6
# print(s)

# Добавление в DataFrame

# df["type"] = ["circle", "square", "triangle"]
# print(df)

# df["value"] = 7
# print(df)

# В DataFrame можно добавить объект Series как строку. Это действие
# относится больше к теме “объединение данных”, но в данном контексте
# она уместна:

# new_row = pd.Series(['yellow', 34, 10, 'rectangle', 7], ['color', 'speed', 'volume', 'type', 'value'])

# print(df.append(new_row, ignore_index=True))


# Удаление из Series

# Для удаления элементов из структуры Series используется метод
# drop(), которому, в качестве аргумента, передается список меток для
# удаления. При этом, необходимо помнить, что при использовании
# данного метода, по умолчанию, текущая структура не изменяется, а
# возвращается объект Series, в котором будут отсутствовать выбранные
# метки.

# print()
# print(s)

# s_new = s.drop(["A", "B"])

# После операции drop(), структура s осталась прежней

# print(s_new)

# Как видно из примера, структура s не изменилась. Вызов метода drop()
# привел к тому, что была создана еще одна структура с именем s_new без
# указанных элементов. Если нужно изменить непосредственно саму
# структуру, то, дополнительно, необходимо аргументу inplace метода
# drop() присвоить значение True:

# s.drop(["A", "B"], inplace=True)
# print(s)


# Удаление из DataFrame

# print()
# print(df)

# df_new = df.drop([0])
# print(df_new)

# Если необходимо модифицировать саму структуру df, то укажите
# дополнительно параметр inplace=True:


# DataFrame - это двумерная таблица, из которой можно удалять не только
# строки, но и столбцы. Для этого необходимо указать ось, с которой мы
# будем работать, она задается через параметр axis, по умолчанию
# axis=0, что означает работу со строками. Если указать axis=1, то это
# позволит удалить ненужные столбцы

# df.drop(["color", "volume"], axis=1, inplace=True)
# print(df)


# Объединение данных

# Использование метода concat

# pd.concat(objs, axis=0, join='outer', join_axes=None,
# ignore_index=False, keys=None, levels=None, names=None,
# verify_integrity=False, copy=True)

# objs: : массив или словарь структур Series, DataFrame.
# Структуры для объединения.

# axis: 0 - строки, 1 - столбцы;
# Ось, вдоль которой будет производиться объединение

# join: {'inner', 'outer'};
# Тип операции объединения, 'outer' - итоговая структура будет
# результатом объединения (логическое ИЛИ) переданных 60
# структур, 'inner' - итоговая структура будет результатом
# пересечения (логическое И) переданных структур

# ignore_index: bool;
# True — не используется значение индекса в процессе
# объединения, False – используется


# dfr1 = pd.DataFrame({"a_type": ['a1', 'a2', 'a3'], 'b_type': ['b1', 'b2', 'b3'],
#                      'c_type': ['c1', 'c2', 'c3']}, index=[0, 1, 2])

# print(dfr1)

# dfr2 = pd.DataFrame({'a_type':['a4', 'a5', 'a6'], 'b_type':['b4',
#                      'b5', 'b6'], 'c_type':['c4', 'c5', 'c6']}, index=[3, 4, 5])

# print(dfr2)

# Теперь объединим эти две структуры в одну:

# df1 = pd.concat([dfr1, dfr2])
# print(df1)

# Создадим ещё одну структуру и объединим ее с первой изменив ось:

# dfr3 = pd.DataFrame({"d_type": ['d1', 'd2', 'd3'], "e_type": ['e1', 'e2', 'e3']})

# print(dfr3)

# df2 = pd.concat([dfr1, dfr3], axis=1)
# print(df2)


# Для выделения в итоговой структуре составляющие компоненты,
# используйте параметр keys при объединении:

# df3 = pd.concat([dfr1, dfr2], keys=["dfr1", "dfr2"])
# print(df3)

# print(df3.loc["dfr2"])


# Если итоговая структура должна являться результатом объединения
# (логическое ИЛИ), то параметру join необходимо присвоить значение
# 'outer'.

# dfr4 = pd.DataFrame({'d_type': ['d2', 'd3', 'd4'], 'e_type': ['e2', 'e3', 'e4']}, index=[1, 2, 3])
# print(dfr4)

# df4 = pd.concat([dfr1, dfr4], axis=1, join='outer')
# print(df4)

# Если итоговая структура должна являться результатом пересечения
# (логическое И), то параметру join необходимо присвоить значение
# 'inner':

# df4 = pd.concat([dfr1, dfr4], axis=1, join="inner")
# print(df4)


# Использование Database-style подхода

# Суть данного подхода в том, что используется очень быстрый способ
# объединения структур данных, который идеологически похож на
# операции с реляционными базами данных.

# pd.merge(left, right, how='inner', on=None, left_on=None,
# right_on=None, left_index=False, right_index=False, sort=True,
# suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)

# left: DataFrame
# “левая” DataFrame структура.

# right: DataFrame
# “правая” DataFrame структура

# how: {'left', 'right', 'outer', 'inner'}; значение по умолчанию: 'inner'
# Один из методов объединения: 'left', 'right', 'outer', 'inner':
# 'left' - это аналог SQL операции 'LEFT OUTER JOIN' при
# этом будут использоваться ключи только из левого DataFrame
# 'right' - аналог SQL операции 'RIGHT OUTER JOIN' - используются ключи из правого DataFrame.
# 'outer' - аналог SQL операции 'FULL OUTER JOIN' - используется объединение ключей из правого и левого
# DataFrame.
# 'inner' - аналог SQL операции 'INNER JOIN' - используется пересечение ключей из правого и левого
# DataFrame.

# on: список
# Список имен столбцов для объединения, столбцы должны
# входить как в левый, так и в правый DataFrame.

# left_on: список
# Список столбцов левого DataFrame, которые будут
# использоваться как ключи.

# right_on: список
# Список столбцов из правого DataFrame, которые будут
# использоваться как ключи.

# left_index: bool; значение по умолчанию: False
# Если параметр равен True, то будет использован индекс (метки
# строк) из левого DataFrame в качестве ключа(ей) для
# объединения

# right_index: bool; значение по умолчанию: False
# Если параметр равен True, то будет использован индекс (метки
# строк) из правого DataFrame в качестве ключа(ей) для
# объединения.

# sort: bool; значение по умолчанию: False
# Если параметр равен True, то данные в полученном DataFrame
# будут отсортированы в лексикографическом порядке.


# Рассмотрим несколько примеров того, как можно использовать функцию merge().

# dfr1 = pd.DataFrame({'k': ['k1', 'k2', 'k3'], 'a_type': ['a1', 'a2', 'a3'],
#                      'b_type':['b1', 'b2', 'b3']})
# print(dfr1)

# dfr2 = pd.DataFrame({'k': ['k1', 'k2', 'k3'], 'c_type': ['c1', 'c2',
#                      'c3']})
# print(dfr2)

# dfr3 = pd.DataFrame({'k': ['k0', 'k1', 'k2'], 'c_type': ['c1', 'c2',
#                      'c3']})
# print(dfr3)

# Объединим их через merge(), в качестве ключа будем использовать столбец k:

# dfm1 = pd.merge(dfr1, dfr2, on="k")
# print(dfm1)

# Пример использования how='left':

# dfm2 = pd.merge(dfr1, dfr3, how="left", on="k")
# print(dfm2)

# Пример использования how='right':

# dfm3 = pd.merge(dfr1, dfr3, how="right", on="k")
# print(dfm3)

# Пример использования how='outer':

# dfm4 = pd.merge(dfr1, dfr3, how="outer", on="k")
# print(dfm4)

# Пример использования how='inner':

# dfm5 = pd.merge(dfr1, dfr3, how="outer", on="k")
# print(dfm5)


#  Работа с внешними источниками данных

# Работа с данными в формате CSV

# CSV (Comma-Separated Values - значения, разделённые запятыми)
# является одним из наиболее популярных форматов для хранения
# табличных данных, представляет собой текстовый документ, в котором
# элементы данных разделены запятыми, а строки файла являются
# записями в таблице


# Чтение данных

# Для загрузки данных из CSV файлов в pandas используется метод
# read_csv(). Количество аргументов в нем достаточно велико, из
# наиболее часто используемых можно выделить следующие:

# filepath_or_buffer: str
# Путь до файла или буфера, который содержит данные в формате CSV

# sep: str; значение по умолчанию: ','
# Разделитель, по умолчанию он равен ',', т.к. в CSV данные
# разделены запятыми. Довольно часто встречается вариант,
# когда данные разделяются табуляцией, так называемые TSVфайлы, если вы используете такой формат, то необходимо
# параметру sep присвоить значение '\t'.

# header: int или список int’ов; значение по умолчанию: 0
# Номер строки, которая содержит имена столбцов загружаемой
# таблицы. По умолчанию header=0. Если header=None, то имена
# столбцов можно передать в параметре names.

# names: массив; значение по умолчанию: None
# Список имен столбцов таблицы, используется, если в файле нет
# строки с именами столбцов и параметр header равен None.

# Данные в формате CSV можно как непосредственно создавать в Python
# программе, так и загрузить из файла.

import pandas as pd
from io import StringIO

# csv_d1 = "col_A, Col_B, Col_C\na1, b1, c1\na2, b2, c2"
# df = pd.read_csv(StringIO(csv_d1))

# print(df)

# В приведенном выше примере в первой строке набора данных
# содержатся заголовки колонок таблицы. Но не всегда бывает так. Ниже
# приведен случай, когда в таблице с данными нет информации об именах
# столбцов, эту ситуацию можно исправить, если параметру header
# присвоить None и передать нужные значения через параметр names:

# csv_d2 = "a1, b1, c1\na2, b2, c2\na3, b3, c3"
# df1 = pd.read_csv(StringIO(csv_d2), header=None, names=["type_a", "type_b", "type_c"])

# print(df1)


# Если файл с данными в формате CSV находится на диске, то для его
# загрузки достаточно передать имя файла в качестве первого параметра
# метода read_csv():


# В случае, когда размер файла очень большой и его невозможно
# загрузить за одни раз в память (DataFrame), воспользуйтесь загрузкой по
# частям. Для этого нужно в функцию read_csv() дополнительно передать
# параметр chunksize, через который указывается количество строк,
# которые нужно считать в рамках одной порции. Такой подход позволяет
# работать с DataFrame'ами как с итераторами


# Запись данных

# Для записи данных в формате CSV используется метод to_csv()

# path_or_buf: str или handle файла; значение по умолчанию: None
# Имя файла или буфера, в котором будут сохранены данные в формате CSV

# sep: str; значение по умолчанию: ','
# Разделитель элементов данных, по умолчанию sep=','

# header: bool или список строк; значение по умолчанию: True
# Если в качестве значения передается True, то в файл первой
# строкой запишутся имена столбцов, взятые из структуры
# данных. Если будет передан список строк, то имена столбцов
# будут взяты из него.

# encoding: str
# Кодировка. Если вы используете Python 3, то по умолчанию она равна 'utf-8'


# csv_data = "col_A, col_B, col_C\na1, b1, c1\na2, b2, c2"
# df = pd.DataFrame(StringIO(csv_data))

# df.to_csv("tmp.csv")


#  Работа с данными в формате JSON

# Чтение данных

# Для чтения данных в формате JSON используется метод read_json()

# path_or_buf: JSON строка или файл; значение по умолчанию: None
# ◦ Путь (это может быть как файл на диске, так и URL) до JSON
# файла или строка, содержимое которой - корректный JSON.

# orient: str; значение по умолчанию: None
# Ориентация, для того, чтобы загружаемый JSON мог быть
# преобразован в структуру данных pandas он должен иметь
# определенный вид. Далее представлены возможные значения
# orient и соответствующий им JSON:
# 'split' : словарь со структурой {index -> [index],
# columns -> [columns], data -> [values]}

# 'records': список со структурой [{column ->
# value}, ... , {column -> value}]

# 'index': словарь со структурой {index -> {column -> value}}

# 'columns': словарь со структурой {column -> {index -> value}}

# 'values': массив значений.

# typ: тип объекта для записи; значение по умолчанию: 'frame'
# Тип структуры pandas, 'series' - это Series, 'frame' -
# DataFrame. В зависимости от значения typ, можно использовать
# определенные значения orient. Если typ='series', то orient
# может быть 'split', 'records' или 'index', если typ='frame',
# то orient нужно выбрать из следующего списка: 'split',
# 'records', 'index', 'columns', 'values'.


# orient='split'
# Для orient='split' формат данных JSON должен выглядеть следующим образом:

# {
#     'columns': ['col_A', 'col_B', 'col_C'],
#     'index': [0, 1],
#     'data': [['a1', 'b1', 'c1'], ['a2', 'b2', 'c2']]
# }

# orient='records'
# JSON данные:
# [
#    {
#       'col_A': 'a1',
#       'col_B': 'b1',
#       'col_C':'c1'
#    },
#    {
#       'col_A': 'a2',
#       'col_B': 'b2',
#       'col_C': 'c2'
#    }
# ]


# orient='index'

# JSON данные:
# {
#     0: {'col_A': 'a1', 'col_B': 'b1', 'col_C':'c1' }
#     1: {'col_A': 'a2', 'col_B': 'b2', 'col_C':'c2' }
# }

# orient='columns'
# JSON данные:
# {
#    'col_A': {'0': 'a1', '1': 'a2'},
#    'col_B': {'0': 'b1', '1': 'b2'},
#    'col_C': {'0': 'c1', '1': 'c2'}
# }

# orient='values'
# JSON данные:
# [
#    ['a1', 'b1', 'c1'],
#    ['a2', 'b2', 'c2']
# ]


# Запись данных

# При работе с JSON довольно часто приходится преобразовывать уже
# готовые структуры данных в этот формат. Для этого используется
# функция to_json().


# Два самых важных аргумента данного метода - это path_or_buf и
# orient, их назначение тоже, что и в методе read_json(), только сейчас
# речь идет о записи данных, т.е. мы указываем файл или буфер, в
# который будут помещены данные.

# d = {"color": ["red", 'green', 'blue'], 'speed': [56, 24, 65], 'volume': [80, 65, 50]}
# df = pd.DataFrame(d)
# print(df)

# В зависимости от того, какого вида JSON файл (буфер) мы хотим
# получить, необходимо параметру orient присвоить соответствующее
# значение, рассмотрим различные варианты.

# orient='split':
# json_split = df.to_json(orient="split")
# print(json_split)

# orient='records':
# json_records = df.to_json(orient="records")
# print(json_records)

# orient='index':
# json_index = df.to_json(orient="index")
# print(json_index)

# orient='columns':
# json_columns = df.to_json(orient="columns")
# print(json_columns)

# orient='values':
# json_values = df.to_json(orient="values")
# print(json_values)


# Работа с Excel файлами

#  Чтение данных

# Для чтения данных используется метод read_excel(). Он может
# работать как с файлами в формате Excel 2003 (расширение .xls), так и с
# файлами в формате Excel 2007 (расширение .xlsx).

# Обратите внимание на параметр header. По умолчанию он равен нулю -
# это означает, что заголовки столбцов лежат в строке с номером 0. В
# нашем случае у таблиц нет заголовков, поэтому нужно параметру header
# присвоить значение None.


# Для работы с Excel-файлом можно использовать класс ExcelFile,
# объекты которого связываются с определенным файлом на диске.
# Объекты этого класса являются контекстными менеджерами, что делает
# возможным работу с конструкцией with:

# with pd.ExcelFile('c:\\test.xls') as excel
# df1 = pd.read_excel(excel, sheetname = "Sheet1", header=None)
# print(df1)

# Запись данных

# Запись осуществляется с помощью метода to_excel()

# Если параметру header присвоить значение None, то в таблице будет
# отсутствовать заголовки колонок

# Для того, чтобы убрать индексы (имена) строк нужно дополнительно
# добавить параметр index=False


# Операции над данными

# d = {"0": [10, 40], "1": [20, 50], "2": [30, 60]}
# df1 = pd.DataFrame(d)
# print(df1)

# d1 = {"0": [12, 16], "1": [24, 54], "2": [14, 25]}
# df2 = pd.DataFrame(d1)
# print(df2)

# Рассмотренные ниже методы не модифицируют саму структуру, они
# возвращают новую структуру, которую можно сохранить в отдельной переменной

# Для сложения структур используется метод add():

# print(df1.add(df2))

# К элементам структуры можно добавить константу

# print(df1.add(5))

# Вычитание осуществляется с помощью метода sub()

# print(df1.sub(df2))

# print(df1.sub(7))

# Для умножения структур применяется метод mul():

# print(df1.mul(df2))

# print(df1.mul(2))

# Для деления используется метод div():

# print(df1.div(df2))

# print(df1.div(2))


# Логические операции

# По имеющимся структурам можно строить новые, элементами которых
# будут логические переменных, значения которых определяются тем,
# удовлетворяет ли элемент исходной структуры определенному условию или нет

# d = {"0": [12, 16], "1": [24, 54], "2": [14, 25]}
# df1 = pd.DataFrame(d)
# print(df1)

# Определим элементы структуры df2, значения которые больше 20:
# print(df1 > 20)

# Pandas предоставляет инструменты свертки структур данных для
# получения сводной информации. Для выполнения операции “логическое
# ИЛИ” по строкам или столбцам используется метод any(). Выбор
# направления определяется параметром axis.

# Свертка по столбцам:
# print((df1 > 20).any())

# Свертка по строкам:
# print((df1 > 20).any(axis=1))


# Для выполнения операции “логическое И” по строкам или столбцам
# используется метод all(). Выбор направления также определяется
# параметром axis.

# Свертка по столбцам
# print((df1 > 20).all())

# Свертка по строкам:
# print((df1 > 20).all(axis=1))

# d = {"0": [12, 16], "1": [17, 54], "2": [18, 68]}
# df = pd.DataFrame(d)
# print(df)

# Если сравнить эти структуры с помощью оператора == , то в результате
# получим DataFrame, элементами которого будут логические переменные.
# Если значения элементов на данной позиции в обоих DataFrame’ах
# совпадают, то переменная будет равна True, в противном случае False:

# print(df2 == df)

# Можно сравнивать каждый элемент структуры с некоторым константным значением:

# print(df2 == 12)

# Для быстрой проверки равенства двух структур используется метод
# equals(). Если структуры равны (на одних и тех же позициях стоят одни
# и те же значения), то результат будет True, в противном случае False:

# print(df2.equals(df))
# print(df2.equals(df2))


# Статистики

# Методы для расчета статистик

# count - количество не-NaN объектов
# sum - сумма
# mean - среднее значение
# mad - среднее абсолютное отклонение
# median - медиана
# min - минимум
# max - максимум
# mode - мода
# abs - абсолютное значение
# prod - произведение
# std - стандартное отклонение
# var - несмещенная дисперсия
# sem - стандартная ошибка среднего
# skew - скошенность (момент 3-го порядка)
# kurt - эксцесс (момент 4-го порядка)
# quantile - квантиль (%)
# cumsum - кумулятивная сумма (промежуточная)
# cumprod - кумулятивное произведение
# cummax - кумулятивный максимум
# cummin - кумулятивный минимум

# d = {"0": [12, 16, 65], "1": [24, 54, 35], "2": [14, 25, 12], "3": [17, 83, 72]}
# df = pd.DataFrame(d)
# print(df)


# Сумма по столбцам:
# print(df.sum())

# Сумма по строкам:
# print(df.sum(axis=1))

# Среднее значение по столбцам:
# print(df.mean())

# Медиана по столбцам:
# print(df.median())

# # Кумулятивная сумма по столбцам:
# print(df.cumsum())

# Для получения сводной информации по статистикам можно
# воспользоваться методом describe():

# print(df.describe())


# Завершим обзор возможностей pandas для расчета статистик функцией
# value_counts(). Для начала создадим список из тридцати значений,
# каждое из которых является случайным числом в диапазоне от 0 до 7:

# import random
# random.seed(123)

# rnd_list = [random.randint(0, 7) for i in range(30)]

# s = pd.Series(rnd_list)
# print(s.value_counts())

# Как видно, больше всего в нашем массиве нулей и единиц.


# Функциональное расширение

# Потоковая обработка данных

# Потоковая обработка данных - это подход, который позволяет в удобном
# виде задавать обработку данных таким образом, что структура pandas
# является аргументом некоторой функции, результат которой передается
# в следующую функцию и т.д. Это похоже на конвейерную обработку. Для
# использования данного подхода применяется метод pipe(). Рассмотрим
# это на примере. Для начала реализуем такую обработку без
# использования функции pipe().

# d = {"0": [12, 16, 65], "1": [24, 54, 35], "2": [14, 25, 12], "3": [17, 83, 72]}
# df = pd.DataFrame(d)
# print(df)

# Создадим три функции: возведение в квадрат, корень третьей степени и
# функция, вычитающая число 10

# sqr = lambda x: x ** 2
# root3 = lambda x: x ** (1.0 / 3.0)
# munis10 = lambda x: x - 10

# Если мы хотим сначала возвести каждый элемент структуры df в
# квадрат, потом вычесть из полученного значения 10, а следом взять
# корень третьей степени, то самый простой способ реализации такой
# задачи выглядит так:

# print(root3(munis10(sqr(df))))

# С такой записью есть несколько проблем: во-первых: она не очень
# наглядная (хотя не все с этим согласятся), во-вторых: у данных функций
# могут быть дополнительные аргументы, что будет затруднять чтение и
# понимание смысла строки кода; в-третьих: модификация такой записи,
# при большом количестве используемых функций, вызовет трудности.

# Обойти эти затруднения можно с помощью функции pipe(), аргументом
# которой является обрабатывающая функция:

# print((df.pipe(sqr)
#          .pipe(munis10)
#          .pipe(root3)))


# Применение функции к элементам строки или столбца

# Pandas предоставляет возможность использовать свои собственные функции
# применяя аналогичный подход. Для этого используется метод apply().

# print(df.apply(lambda x: sum(x)/len(x)))

# Вот вариант функции, которая вычисляет квадратный корень из суммы по столбцам:
# print(df.apply(lambda x: sum(x)**(0.5)))

# Такую же операцию можно сделать построчно
# print(df.apply(lambda x: sum(x)**(0.5), axis=1))


# Агрегация (API)

# d = {"0": [12, 16, 65], "1": [24, 54, 35], "2": [14, 25, 12], "3": [17, 83, 72]}
# df = pd.DataFrame(d)
# print(df)

# Ещё одним функциональным расширением, которое предоставляет
# библиотека pandas является агрегация. Суть в том, что можно
# использовать несколько функций в том виде, как это мы делали в
# предыдущем разделе, когда разбирали функцию apply(), только в этом
# случае нам поможет функция agg().

# print(df.agg("sum"))

# Но что если мы хотим сразу посчитать сумму, среднее значение и
# стандартное отклонение? Используя агрегацию это очень просто
# сделать:

# print(df.agg(["sum", 'mean', 'std']))

# Не забудем и про свои собственные функции:

# strange = lambda x: sum(x)**(0.5)
# min_div_5 = lambda x: min(x) / 5.0

# print(df.agg([max, strange, min_div_5]))

# Как вы заметили, в данном примере, на месте имен функций strange и
# min_div_5 стоит надпись <lambda>, чтобы это убрать перепишем эти
# функции как обычные python-функции:

# def strange(x):
#     return sum(x)**(0.5)

# def min_div_5(x):
#     return min(x) / 0.5

# print(df.agg([max, strange, min_div_5]))


# Теперь все в порядке! Если вернуться к предыдущему варианту с
# lambda-функциями, можно добиться нужного нам результата присвоив
# функциям соответствующие имена (в качестве имени может выступать
# любая строка):

# strange = lambda x: sum(x)**(0.5)
# min_div_5 = lambda x: min(x) / 5.0

# strange.__name__ = "strange fun"
# min_div_5.__name__ = "min / 5"

# print(df.agg([max, strange, min_div_5]))


# Трансформирование данных

# Формат вызова трансформирующей функции похож на тот, что
# используется при агрегации. В отличии от последней, трансформация -
# это применение функции к каждому элементу структуры, в результате
# будет возвращена структура того же размера (или больше), но с
# измененными элементами.


# d = {"0": [12, 16, 65], "1": [24, 54, 35], "2": [14, 25, 12], "3": [17, 83, 72]}
# df = pd.DataFrame(d)
# print(df)

# Создадим две функции:

# mul2 = lambda x: x * 2
# mul2.__name__ = "mul2"
# div2 = lambda x: x / 2
# div2.__name__ = "div2"


# Используем трансформирующую функцию для модификации элементов
# структуры df

# print(df.transform([mul2]))


# Вариант с двумя функциями для модификации:

# print(df.transform([mul2, div2]))

# Можно работать с отдельными строками

# print((df.iloc[0]).transform([mul2, div2]))


# Использование методов типа str для работы с текстовыми данными

# Часто структуры данных pandas используются для хранения текстовых
# данных - строк и символов. Текстовые данные также могут быть частью
# самой структуры, например заголовки столбцов и т.п. Pandas позволяет
# использовать функционал типа str языка Python для работы с такими
# данными.


# s = pd.Series([" hellO", " abcABC", " one", "TWO ", " tHRee"])
# print(s)

# Приведем все буквы элементов структуры s к нижнему регистру:

# (print(s.str.lower()))

# Удалим все лишние пробелы и сделаем заглавными первые буквы слов:

# print(s.str.lower().str.strip().str.title())


# Настройка pandas

# API для работы с настройками pandas

# Функции для работы с настройками pandas

# get_option() - получени значения параметра
# set_option() - установка нового значения параметра
# reset_option() - сброс параметра на значение "по умоланию"
# describe_option() - вывод текстового описания параметра
# option_context() - присвоение параметрам новых значений в рамках определенного блока кода.
# Используется с оператором with


# Также возможна работа с настройками как с атрибутами (через точку)

# Рассмотрим первый вариант: изменение настроек через вызов функции.
# Для получения значения параметра используется функция
# get_option(), в качестве аргумента ей передается настраевыемый параметр в текстовом виде.

# Максимальное количество выводимых строк:

# print(pd.get_option("display.max_rows"))

# Используемая кодировка:

# print(pd.get_option("display.encoding"))

# Максимальное количество выводимых столбцов с данными:

# print(pd.get_option("display.max_columns"))


# Установка нового значения осуществляется с помощью функции
# set_option(): ее первым аргументом является название параметра в
# текстовом виде, вторым - новое значение параметра.

# s = pd.Series([10, 3, 46, 1, 312, 144, 193, 42, 39, 77, 3])
# print(s)

# Текущее значение параметра display.max_rows (максимальное
# количество выводимых строк):
# print(pd.get_option("display.max_rows"))

# Изменим это значение на 5:
# pd.set_option("display.max_rows", 5)
# print(pd.get_option("display.max_rows"))
# print(s)

# Установим прежнее значение
# pd.set_option("display.max_rows", 60)
# print(pd.get_option("display.max_rows"))
# print(s)

# Значение параметра можно сбросить на значение “по умолчанию”, для
# этого используется функция reset_option().

# Исходное значение display.max_rows:
# print(pd.get_option("display.max_rows"))

# Присвоим новое значение параметру display.max_rows=10:
# pd.set_option("display.max_rows", 10)
# print(pd.get_option("display.max_rows"))

# Сбросим display.max_rows на значение “по умолчанию”:
# pd.reset_option("display.max_rows")
# print(pd.get_option('display.max_rows'))


# Описание параметра можно получить с помощью функции
# describe_option():
# print(pd.describe_option("display.max_rows"))

# Если необходимо выполнить блок кода, в рамках которого нужно
# временно присвоить ряду настроенных параметров определенные
# значения, то в этом случае рекомендуется использовать функцию
# option_context():

# with pd.option_context("display.max_rows", 25):
#     print(pd.get_option("display.max_rows"))


# print(pd.get_option("display.max_rows"))


# Второй вариант работы с настройками — это использование свойства
# options. Выведем значения уже знакомых нам настроек:

# print(pd.options.display.max_rows)
# print(pd.options.display.max_columns)

# Присвоим параметру новое значение:
# pd.options.display.max_columns = 5
# print(pd.options.display.max_columns)


# Настройки библиотеки pandas

# Настраиваемые параметры pandas

# display.chop_threshhold - None - если float-значение ниже этого порога, то оно будет отображаться равным 0
# display.colheader_justify - right - выравнивание заголовков столбцов. Используется DataFrameFormatter'ом
# display.date_dayfirst - False - Если параметр равен True, то при отображении и парсинге дат будет использоваться следующий порядок: день/месяц/год
# display.date_yearfirst - False - Если параметр равен True, то при отображении и парсинге дат будет использоваться следующий порядок: год/месяц/день
# display.encoding - UTF-8 - Определяет кодировку для строк, возвращаемых через метод to_string, а также для отображения строк в консоли.
# display.max_columns - 20 - Если Python запущен в терминале, то данный параметр может быть установлен в 0, в этом случае pandas будет автоматически
# определять ширину терминала и использовать другой формат представления, если все столбцы не поместились вертикально.
# Значение None определяет неогриниченное количество символов
# display.max_colwidth - 50 - Максимальная ширина столбца для представления структур данных pandas. Когда происходит переполнение столбца по символам, то в вывод будут добавлены символы "..."
# display.max_rows - 60 - Параметр устанавливает максимальное количество строк, которые будут выведены при отображении структур pandas
# display.memory_usage - True - Указывает, должен ли отображаться объем занимаемой памяти для DataFrame при вызове метода df.info()
# display.notebook_repr_html - True - Если параметр равен True, то IPython notebook будет использовать html-представление для объектов pandas.
# display.precision - 6 - Определяет количество знаков после запятой при выводе чисел с плавающей точкой.
# display.width - 80 - Ширина дисплея в символах. Если python/IPython запущен в терминале, то параметр может быть установлен в None, в этом случае pandas автоматически определит ширину.
# display.html.border - 1 - Определяет значение для атрибута border=value, который будет вставлен в тег <table> для HTML- представления структуры DataFrame
# io.excel.xls.writer - xlwt - Используемый по умолчанию движок для работы с Excel-файлами в формате "xls"
# io.excel.xlsm.writer - openpyxl - Используемый по умолчанию движок для работы с Excel-файлами в формате "xlsx"
# io.excel.xlsx.writer - openyxl - Используемый по умолчанию движок для работы с Excel-файлами в формате "xlsx"
# mode.sim_interactive - False - Моделирование интерактивного режима, используется при тестированни


# Глава 9. Инструменты для работы с данными


# Скользящее окно. Статистики

# Первый набор инструментов, который мы рассмотрим, относится к
# группе, которую можно назвать скользящие статистики (или оконные
# функции). Суть их заключается в том, что различные статистики, такие
# как математическое ожидание, медиана, ковариация, стандартное
# отклонение и т.п. расчитываются не для всех объектов структуры, а
# только для группы подряд идущих значений, размер этой группы
# предварительно задается задавать вручную (этот параметр носит
# название - размер окна).


# import random

# arr = [random.randint(0, 50) for i in range(500)]

# s = pd.Series(arr)
# print(s.shape)
#
# print(s[0:5])

# Для расчета различных статистик с заданным окном, для начала,
# создадим объект класса Rolling. Для этого воспользуемся функцией
# pandas.DataFrame.rolling():

# DataFrame.rolling(window, min_periods=None, freq=None, center=False,
# win_type=None, on=None, axis=0, closed=None)

# window: int
# Размер окна

# min_periods: int; значение по умолчанию: None
# Минимальное количество элементов в окне для получения значения статистики.

# freq: str или объект DateOffset; значение по умолчанию: None
# В настоящий момент данный параметр имеет статус
# Deprecated и не рекомендован к использованию.

# center: bool; значение по умолчанию: False
# Установка метки в центр окна.

# win_type: str; значение по умолчанию: None
# Тип окна

# on: str; значение по умолчанию: None
# Задает столбец, по которому будут производиться вычисления.

# closed: str; значение по умолчанию: None
# Определяет конечные точки закрытого интервала ('right',
# 'left', 'both', 'neither').

# axis: int или str; значение по умолчанию: 0
# Ось, вдоль которой будут производиться вычисления (0 -
# вычисления по строкам, 1 - по столбцам).


# Создадим объект Rolling с настройками по умолчанию и зададим
# размер окна равным 10:

# roll = s.rolling(window=10)
# print(roll)

# Данный объект предоставляет методы для расчета скользящих статистик.


# Методы, используемые для расчета скользящих статистик

# apply() - функция общего назначения
# corr() - корреляция
# cov() - ковариация
# count() - количество не-NaN объектов
# kurt() - эксцесс (момент 4-го порядка)
# var() - несмещенная дисперсия


# Теперь можно использовать функции расчета статистик применительно к
# созданному набору данных с заданным окно

# pd.options.display.max_rows = 20
# print(roll.median())

# Как вы можете видеть: до десятого
# элемента (в нашем случае, это элемент с индексом 9 т.к. счет
# начинается с нуля) элементы структуры имеют значение NaN. Это связно
# с тем, что размер окна равен 10, и статистика считается по десяти
# предыдущим элементам, такой момент наступает, когда мы доходим до
# индекса 9

# Вычисление стандартного отклонения для окна размера 10:

# print(roll.std())

# Определение элемента с максимальным численным значением среди
# элементов окна:

# print(roll.max())


# Pandas позволяет задать тип окна, это делается с помощью параметра
# win_type функции rolling. Ниже приведена таблица возможных
# значений данного параметра.

# Параметры, определяющие тип окна для расчета статистик

# boxcar - прямоугольно окно (окно Дирихле)
# triang - треугольное окно
# blackman - окно Блэкмана
# hamming - окно Хемминга
# bartlett - окно Барлетта
# parzen - окно Парзена
# borhman - окно Бохмана
# blackmanharris - окно Блэкмана-Харриса
# nuttall - окно Наттела
# barthann - окно Барлетта-Ханна
# kaiser - окно Кайзера
# gaussian - окно Гаусса
# general_gaussian - обобщенное Гауссовое окно
# slepian - окно Слепиана


# Расширяющееся окно. Статистики

# Этот раздел посвящен расширяющимся
# окнами, размер которых, в отличии от скользящих, изменяется,
# расширяясь, начиная с первого элемента.


# Для работы с расширяющимся окном используется объект класса
# Expanding, который можно получить через метод expanding() структуры
# pandas.

# print(s[0:5])

# ex = s.expanding()
# print(ex)

# pd.options.display.max_rows = 20
# print(ex.mean())

# Вычисление стандартного отклонения для расширяющегося окна:

# print(ex.std())

# Подход к работе с расширяющимся окном аналогичен тому, что
# используется для скользящего окна.


# Время-ориентированное скольжение

# Суть время-ориентированного скольжения в том, что в качестве окна
# используется временной интервал.

# df = pd.DataFrame([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], index=pd.date_range("20210101 00:00:00", periods=10, freq="s"))
# print(df)

# Создадим объект Rolling с окном в 2 секунды:
# rt = df.rolling("2s")
# print(rt.sum())

# Вот такой результат будет для окна в 5 секунд:
# rt = df.rolling("5s")
# print(rt.sum())


# Агрегация данных

# Функция агрегации позволяет одновременно производить расчет
# различных статистик для заданного набора данных.

# df = pd.DataFrame([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], index=pd.date_range("20210101 00:00:00", periods=10, freq="s"))
# print(df)

# Создадим для него объект скольжение:

# rt = df.rolling(window="5s")
# print(rt)

# Теперь посчитаем одновременно сумму, среднее значение и
# стандартное отклонение. Функции для расчета статистик возьмем из
# библиотеки numpy:

# import numpy as np

# print(rt.agg([np.sum, np.mean, np.std]))

# При необходимости, можно самостоятельно реализовать функции,
# рассчитывающие нужные статистики.


# Временные ряды


# Популярным направлением в области работы с данными является
# анализ временных рядов. Библиотека pandas содержит набор
# инструментов, который позволяет производить манипуляции с данными,
# представленными в виде временных рядов.

# Классы pandas, используемые для работы с временными рядами

# Класс  - Создание - Описание
# Timestamp - to_datetime, Timestamp - единичная временная метка
# DatetimeIndex - to_datetime, date_range, bdate_range, DatetimeIndex - Набор объектов Timestamp
# Period - Period - Единичный временной интервал
# PeriodIndex - period_range, PeriodIndex - Набор объектов Period


# В pandas различают временные метки и временные интервалы.
# Временная метка - это конкретное значение даты/времени, например:
# 2018-01-01 01:20:35. Временной интервал предполагает наличие
# неполной временной метки и маркера, который определяет период,
# например метка 2018-01 будет иметь маркер 'M', что означает - месяц.
# DatetimeIndex - это класс, позволяющий хранить массив временных
# меток, которые могут быть использованы в качестве индексов при
# построение структур данных pandas. PeriodIndex - класс, хранящий
# массив временных интервалов, который также может быть использован
# в качестве индекса


# Работа с временными метками

# Создание временной метки

# Для создания временной метки (объекта класса Timestamp) можно
# воспользоваться конструктором Timestamp(), либо методом
# to_datetime().

# Начнем со знакомства с конструктором Timestamp:
# pandas.Timestamp(ts_input, freq, tz, unit, offset)

# ts_input: datetime, str, int, float
# Значение, которое будет преобразовано в объект класса Timestamp

# freq: str, DateOffset
# Величина сдвига.

# tz: str, pytz.timezone, dateutil.tz.tzfile или None; значение по умолчанию: None
# Временная зона.

# unit: str; значение по умолчанию: None
# Единица измерения, параметр используется если значение ts_input имеет тип int или float

# offset: str, DateOffset: значение по умолчанию: None
#  Не поддерживается, используйте freq.


# Рассмотрим варианты создания объекта Timestamp.

# Создание объекта из строки:
# ts = pd.Timestamp("2018-10-5")
# print(ts)

# ts = pd.Timestamp("2018-10-05 01:15:33")
# print(ts)

#  Использование объекта datetime:
# from datetime import datetime

# dt = datetime.now()
# print(dt)

#  Использование int и float значений:
# ts = pd.Timestamp(1517246359, unit="s")
# print(ts)
# ts = pd.Timestamp(3, unit="s")
# print(ts)


# Для создания объектов Timestamp можно использовать метод
# to_datetime(). Это мощный инструмент, который обладает широким
# функционалом, мы не будем рассматривать все его возможности, наша
# задача - просто познакомиться с ним:

# ts = pd.to_datetime("2018-01-01 00:01:02")
# print(ts)

# ts = pd.to_datetime("20180101023215", format="%Y%m%d%H%M%S")
# print(ts)

# ts = pd.to_datetime(1517246359, unit="s")
# print(ts)

# Обратите внимание, что при использовании Timestamp можно
# оперировать с временными данным в следующем диапазоне:

# print(pd.Timestamp.min)
# print(pd.Timestamp.max)



# Создание ряда временных меток

# Ряд временных меток - это объект класса DatetimeIndex, который может
# выступать в качестве индекса при создании структур Series и DataFrame.
# Объект такого класса можно создать с помощью уже известного нам
# метода to_datetime(), либо date_range()

# Для начала рассмотрим вариант работы с to_datetime(). Если в
# качестве аргумента передать данному методу список строк, чисел (int
# или float) или объектов Timestamp, то в результате будет создан
# соответствующий объект DatetimeIndex:

# dti = pd.to_datetime(["2018-01-01", "2018-01-02"])
# print(dti)

# Можно использовать список объектов Timestamp:

# dti = pd.to_datetime([pd.Timestamp("2018-01-01"), pd.Timestamp("2018-01-02")])
# print(dti)

# dti = pd.to_datetime([1517246359, 1517246360, 1517246361], unit='s')
# print(dti)


# Для создания временного ряда из заданного диапазона применяется
# метод date_range():

# pandas.date_range(start=None, end=None, periods=None, freq='D', tz=None,
# normalize=False, name=None, closed=None, **kwargs)

# start: str; значение по умолчанию: None
    # Левая граница генерируемых данных

# end: str; значение по умолчанию: None
    # Правая граница генерируемых данных

# period: integer; значение по умолчанию: None
    # Количество элементов в создаваемом массиве.

# freq: str(или DateOffset); значение по умолчанию: 'D'
    # Шаг, с которым будут генерироваться данные

# tz: str; значение по умолчанию: None
    # Временная зона, например: “Europe/Brussels”.


# Метод date_range() возвращает объект класса DatetimeIndex, который
# можно использовать в качестве индекса при построении структур данных
# pandas.

# Возможные значения параметра freq, определяющего шаг при
# создании диапазона методом date_range()
# Значение параметра freq - Описание
# D - день
# W - неделя
# M - месяц
# SM - половина месяца (15 дней)
# Q - квартал
# A, Y - Год
# H - час
# T, min - минута
# S - секунда
# L, ms - миллисекунда
# U, ms - микросекунда
# N - наносекунда




# Создадим массив временных меток с шагом в один час, который
# охватывает интервал в пять дней:

# dt_h = pd.date_range(start="2017-02-01", freq="H", periods=120)
# print(dt_h[:30])

# Таким же образом можно создать DatetimeIndex, содержащий
# временные метки с шагом в одну минуту и общим интервалом в один
# час:
# dt_m = pd.date_range(start="2017-02-01", freq="min", periods=60)
# print(dt_m[:30])

# print(len(dt_m))
# print(len(dt_h))

# Следующим нашим шагом будет создание структуры Series, в которой, в
# качестве индексов, будет использоваться первый, созданный нами
# DatetimeIndex:

# import random

# rnd = [random.randint(-5, 5) for i in range(len(dt_h))]
# s_dt = pd.Series(rnd, index=dt_h)
# pd.options.display.max_rows = 20
# print(s_dt)



# Работа с временными интервалами

# Единичный временной интервал - это объект класса Period. Его можно
# создать, используя одноименный конструктор, основными параметрами
# которого являются value и freq:

# Единичный временной интервал - это объект класса Period. Его можно
# создать, используя одноименный конструктор, основными параметрами
# которого являются value и freq:

# value: str; значение по умолчанию: None
     # Временной период

# freq: str; значение по умолчанию: None
     # Строка с меткой, определяющей временной интервал.


# Ниже приведены примеры того, как можно конструировать объекты
# Period:

# print(pd.Period("2018"))
# print(pd.Period("2018-01"))
# print(pd.Period("2018", "M"))

# После того, как создан объект класса Period с ним можно производить
# различные арифметические действия. Например: создадим объект с
# интервалом в день:

# prd = pd.Period("2018", freq="D")
# print(prd)

# Теперь прибавим к нему число 7, что будет означать прибавление семи
# дней к текущей дате:

# print(prd + 7)

# Если прибавить число большее 31, то увидим, что изменился месяц:
# print(prd + 53)

# Точно также, если мы создадим объект, в котором интервалом будет
# месяц, то арифметические операции будут производится над месяцами
# относительно указанной даты:

# prd_m = pd.Period("2018", freq="M")
# print(prd_m)

# print(prd_m + 5)



# Создание ряда временных интервалов

# Массивы временных интервалов могут использоваться при построении
# структур данных pandas. Для работы с такими массивами существует
# специальный класс - PeriodIndex

# Создание объекта такого класса осуществляется посредством
# конструктора PeriodIndex() или метода period_range().


print(pd.PeriodIndex(["2018", "2017", "2016"], freq="M"))

print(pd.PeriodIndex(['2018', '2017', '2016'], freq='D'))


# Более удобным инструментом для создания рядов временных
# интервалов является метод period_range(), который, по принципу
# работы с ним, похож на date_range() из раздела “Создание ряда
# временных меток”:

# pandas.period_range(start=None, end=None, periods=None, freq='D',
# name=None):

# start: str; значение по умолчанию: None
    # Левая граница генерируемых данных.

# end: str; значение по умолчанию: None
    # Правая граница генерируемых данных.

# period: integer; значение по умолчанию: None
    # Количество элементов в массиве.

# freq: str (или DateOffset); значение по умолчанию: 'D'
    # Шаг, с которым будут генерироваться данные

# name: str; значение по умолчанию: None
    # Имя объекта PeriodIndex.


# Ряд временных интервалов в диапазоне между 2018 и 2019-м годом с шагом в один месяц:

# print(pd.period_range("2018", "2019", freq="M"))

# Ряд временных интервалов в диапазоне между 2018 и 2019-м годом с
# шагом в один день:

# print(pd.period_range("2018", "2019", freq="D"))


# Начиная с 2018.01.01 построить десять временных интервалов с шагом
# в неделю:

# print(pd.period_range("2018-01-01", periods=10, freq="W"))


# Использование временных рядов в качестве индексов

# Создадим DatetimeIndex - ряд временных меток с отсечкой в один день:

# dt_d = pd.period_range(start="2017-02-01", freq="D", periods=50)
# print(dt_d)

# На базе полученного объекта создадим структуру Series
# import random
# dr = [random.randint(-10, 10) for i in range(len(dt_d))]
# s = pd.Series(dr, index=dt_d)

# Для получения доступа к структуре можно использовать числовые
# индексы:

# print(s[:5])

# Либо метку времени в текстовом виде:
# print(s["2017-02-01"])

# Для получения данных из заданного временного диапазона допускается
# использование срезов:

# print(s["2017-02-01":"2017-02-10"])


# Использование временных рядов в качестве индексов, предоставляет
# дополнительные возможности по выборке данных. Получим данные за
# февраль 2017 года из созданной выше структуры s:

# pd.options.display.max_rows = 10
# print(s["2017-02"])


# Вместо строкового представления даты допускается применять объекты
# datetime:
# from datetime import datetime
# print(s[datetime(2017,2,1):datetime(2017,2,8)])